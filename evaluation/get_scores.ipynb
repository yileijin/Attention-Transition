{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5c8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aecb4f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e67ca428",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = ''\n",
    "with open(file_path) as df:\n",
    "    data = json.load(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fcfec",
   "metadata": {},
   "source": [
    "###  Get Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84886cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(doc):\n",
    "    scores = []\n",
    "    clips = [clip.strip() for clip in doc['gpt4_eva'].split(':')]\n",
    "    for clip in clips:\n",
    "        if clip != '':\n",
    "            if clip[0] == '0':\n",
    "                scores.append(int(clip[0]))\n",
    "            elif clip[:2].isdigit():\n",
    "                if '100' in clip:\n",
    "                    scores.append(100)\n",
    "                else:\n",
    "                    scores.append(int(clip[:2]))\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07dd1c",
   "metadata": {},
   "source": [
    "### Find outlier\n",
    "In our results, these procedures have already been done, you can just skip them and directly see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d74e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some generation results are not the normal format, which may cause truble when computing scores.\n",
    "\n",
    "outlier_doc = []\n",
    "scores = []\n",
    "valid_keys = int()\n",
    "for i in range(len(data)):\n",
    "    score = get_scores(doc=data[i])\n",
    "    scores.append(get_scores(doc=data[i]))\n",
    "    if len(score) != valid_keys:\n",
    "        outlier_doc.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outlier_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14961f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = []\n",
    "for i in range(len(scores)):\n",
    "    if len(scores[i]) != 0:\n",
    "        score = scores[i]\n",
    "        score_or = score[0]\n",
    "        score_tr = max(score[1:])\n",
    "        score_tuple = (score_or, score_tr)\n",
    "        eval_scores.append(score_tuple)\n",
    "    else:\n",
    "        print(i)\n",
    "        eval_scores.append((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c32e55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i]['eval_scores'] = eval_scores[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238a2f3",
   "metadata": {},
   "source": [
    "### Calculate Final Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d197659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This choose the original generation results and the best generation results\n",
    "## The outcomes present in main results \n",
    "\n",
    "def compare_or_tr(data):\n",
    "    score_or = 0\n",
    "    score_tr = 0\n",
    "    for doc in data:\n",
    "        score_or += doc['eval_scores'][0]\n",
    "        score_tr += doc['eval_scores'][1]\n",
    "    score_or /= len(data)\n",
    "    score_tr /= len(data)\n",
    "    return score_or, score_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_or_tr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a984eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This choose the original generation results and the average generation results for each set of parameters\n",
    "## The outcomes present in ablation study results \n",
    "\n",
    "def all_aver_score(data):\n",
    "    ### get scores and outlier doc\n",
    "    outlier_doc = []\n",
    "    scores = []\n",
    "    for i in range(len(data)):\n",
    "        score = get_scores(doc=data[i])\n",
    "        scores.append(get_scores(doc=data[i]))\n",
    "        if len(score) != 25:\n",
    "            outlier_doc.append(i)\n",
    "    \n",
    "    ### get exact score for each kind of generation\n",
    "    ## get keys\n",
    "    keys = [key for key in data[0].keys()]\n",
    "    keys = keys[3:-2]\n",
    "    print(keys,len(keys))\n",
    "    ## get score dict\n",
    "    score_dict = {}\n",
    "    for key in keys:\n",
    "        score_dict[f'{key}'] = 0\n",
    "        \n",
    "    ### count all score\n",
    "    keys = keys[:-1]\n",
    "    length = len(data) - len(outlier_doc)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i not in outlier_doc:\n",
    "            score_dict['decode_or'] += scores[i][0] / length\n",
    "            for j in range(len(scores[i][1:])):\n",
    "                score_dict[f'{keys[j-1]}'] += scores[i][j] / length\n",
    "                           \n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aver_score(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
